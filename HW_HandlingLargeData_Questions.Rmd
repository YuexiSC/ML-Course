---
title: 'HW on Handling Large Datasets: Questions'
output: html_document
editor_options: 
  chunk_output_type: console
---

Turn in your HW on this Rmarkdown file. Add your names here at the top. Your answers should be neatly arranged in blocks of code below; insert code blocks as needed. Use the markdown language to add any text you need to explain your homework. 

##Getting and Handling Large Data Sets
```{r}
load("MDHtickers.RData")
setwd("/Users/liyuexi/Documents/Project/Homework/HW2")
```
####GET THE DATA

You will acquire and analyze a real dataset on baby name popularity provided by the Social Security Administration. To warm up, we will ask you a few simple questions that can be answered by inspecting the data.

The data can be downloaded in zip format from:
http://www.ssa.gov/oact/babynames/state/namesbystate.zip  (~22MB)

#### QUESTION 1
Please describe the format of the data files. Can you identify any limitations or distortions of the data.

First, download the zip file and unzip it to grab the contents. You will see lots of different files. Store them in a directory called "namesbystate". Then we write R code to read those files and put them together. 
```{r}
library(data.table)
library(magrittr)
setwd("/Users/liyuexi/Documents/Project/Homework/HW2/namesbystate/")
getwd()

file_list <- list.files(path='/Users/liyuexi/Documents/Project/Homework/HW2/namesbystate/') 
dataset <- do.call("rbind", lapply(file_list, FUN=function(files) {read.table(paste('namesbystate/',files,sep=""), header=FALSE, sep=",")} 
))
colnames(dataset)<- cbind('state','sex','birth_year','name','occurrence')
sample(dataset)
```

#### QUESTION 2
What is the most popular name of all time across both genders? 
```{r}
library(plyr)
aggre_data<-aggregate(occurrence ~ name,dataset,sum)
idx = sort(as.matrix(aggre_data$occurrence),decreasing=TRUE,index.return=TRUE)$ix
head(aggre_data[idx,],15)
```
Answer2:
James

#### QUESTION 3
What is the most gender ambiguous name in 2013? 1945? (you need to come up with an ambiguity metric, so think about how you may want to implement this. You may of course search for definitions of ambiguity.)
```{r}
library(dplyr)
library(plyr)
Ambiguous_name<- function(df,input_year){
  #1.Subset the dataset into gender groups, and inner join two datasets by name. 
  inner_year<-merge( filter(df,birth_year==input_year, sex=='F'), filter(df,birth_year==input_year, sex=='M'), by='name')
  #2.Create a new column named "common" which is the min values of name of occurence in female and male.
  final_year<- subset(inner_year,select=c(name,occurrence.x,occurrence.y)) 
  final_year<- aggregate(final_year[,2:3],list(final_year[,1]),sum)
  colnames(final_year)<- c('name','occurence_F','occurence_M')
  final_year$common<- apply(final_year, 1, FUN=min)
  #3.Define ambiguous metric: By amount of "common"
  idx = sort(as.matrix(final_year$common),decreasing=TRUE,index.return=TRUE)$ix
  head(final_year[idx,],1)
}
#Get results for 1945 & 2013
Ambiguous_name(dataset,1945)
Ambiguous_name(dataset,2013)

```
Answer:
ambiguous name in 1945：Leslie
ambiguous name in 2013：Riley

#### QUESTION 4
Of the names represented in the data, find the name that has had the largest percentage increase in popularity since 1980. Largest decrease?

```{r}
library(dplyr)
library(reshape2)
library(reshape)
library(caTools)
library(magrittr)
library(plyr)
#Question: Names has had largest percentage increase

#1. Aggregate dataset by Name and Birth_year get the total number of occurence 
dataset2<- aggregate(dataset$occurrence~name+birth_year, dataset,sum)
base_name<- dataset2[dataset2$birth_year==1980,]$name

#2. Reshape the table 
idx_year<- list()
for (ix in unique(dataset2$birth_year)) {
    ix<- paste("bir_",ix, sep="")
    idx_year<- c(idx_year,ix)
}

data_dcast<- dcast(dataset2, name ~ birth_year)
data_dcast
colnames(data_dcast)<-c("name",idx_year)
data_shaped<-subset(data_dcast,select=c(name,bir_1980:bir_2016))
data_shaped<-aggregate(.~name, data_shaped, FUN=sum,na.action=NULL)
data_1980<-data_shaped[base_name,]

#3.Iterate each row through a running window to get max increase and min increase and insert into table
data_1980_percentage<-data_1980
for(col_idx in 2:ncol(data_1980)) {
    col=as.numeric(data_1980[,col_idx])
    data_1980_percentage[,col_idx]<-col*100/sum(col,na.rm = TRUE)
    }
min_list<-c()
max_list<-c()
idx <-1  
for(row_idx in 1:nrow(data_1980)) {
    max_increase<-0
    row=as.numeric(data_1980_percentage[row_idx,2:38])
    row_mins<- runmin(row,length(row),align = c("right"))
    max_increase<- max(row-row_mins,na.rm=TRUE)
    max_list[[idx]]<-max_increase
    row_maxs<- runmax(row,length(row),align = c("right"))
    max_decrease<- min(row-row_maxs,na.rm=TRUE)
    min_list[[idx]]<-max_decrease
    idx<-idx+1
}
data_1980$max_increase<- max_list
data_1980$max_decrease<- min_list

#5.Get the Final Largest percentage increase and Largest percentage decrease\
idx_max = sort(as.matrix(data_1980$max_increase),decreasing=TRUE,index.return=TRUE)$ix
head(data_1980[idx_max,],1)
idx_min = sort(as.matrix(data_1980$max_decrease),decreasing=FALSE,index.return=TRUE)$ix
head(data_1980[idx_min,],1)

```
Answers:
Largest percentage of increase: Ashley
Largest percentage of decrease: Jennifer


#### QUESTION 5
Can you identify names that may have had an even larger increase or decrease in popularity? (This requires you to consider every year as the start year and find the greatest increase/descrease across all years. Print out the top name for growth from each year.)
```{r}
library(zoo)
#1.Referring to the Question 4, get the long data shape into wide version.
data_wide<- data_dcast
data_delta_max<-data_wide
data_delta_min<-data_wide
for(row_idx in 1:nrow(data_wide)) {
    row=as.numeric(data_wide[row_idx,-1])
    # print(row)
    row_maxs<- runmax(row,length(row),align = c("right"))
    row_maxs[row_maxs==-Inf]=NA
    row_maxs[row_maxs==Inf]=NA
    # print(row_maxs)
    delta<- (row-row_maxs) 
    data_delta_max[row_idx,-1]<-delta
    # break
}
for(row_idx in 1:nrow(data_wide)) {
    row=as.numeric(data_wide[row_idx,-1])
    # print(row)
    row_mins<- runmin(row,length(row),align = c("right"))
    row_mins[row_mins==-Inf]=NA
    row_mins[row_mins==Inf]=NA
    # print(row_mins)
    delta<- (row-row_mins) 
    data_delta_min[row_idx,-1]<-delta
    # break
}


 
test<-c(NA,3,4,NA,7,2)
test_min=runmin(c,6,align = c("right"))
test_min_1=runmax(test-test_min,6,align = c("right"))
test
test_min
test-test_min
test_min_1

idx_max = sort(as.matrix(data_1980$max_increase),decreasing=TRUE,index.return=TRUE)$ix
head(data_1980[idx_max,],1)
```

This gives interesting results, and may be used in a different way with a rolling window than using all the data. 

#### QUESTION 6

What insight can you extract from this dataset? Feel free to combine the baby names data with other publicly available datasets or APIs, but be sure to include code for accessing any alternative data that you use.

This is an open-ended question and you are free to answer as you see fit. In fact, it would be great if you find a way to look at the data that is highly interesting.
```{r}
#Something related to the baby's name
#2. President name : Fit and Plot 

```

#### QUESTION 7

Go to the airlines data site: 
http://stat-computing.org/dataexpo/2009/the-data.html. 
Read in the airlines data set for 2008 into a data frame.
How many rows of data do you have?
```{r}
airlines<- fread("2008.csv",header=TRUE) 
dim(airlines)
```
Answers:
7009728 rows total

#### QUESTION 8

Remove all rows of the data frame with missing data. How many rows of data do you have now?
```{r}
library(DT)
idx1 = which(rowSums(is.na(airlines))==0)
airlines_na = airlines[idx1,] 
sample(airlines_na)
``` 
Answer:
Has 1524735 rows

#### QUESTION 9

Fit one regression model each to explain "DepDelay" and "ArrDelay". Use your judgment as to which variables you might use to explain these outcomes. Use a subset of 1 million rows of the data you created with no missing data. Keep the remaining data for out-of-sample testing. (**Remember to factor all categorical variables.**)
```{r}
library(caret)
library(lattice)
library(ggplot2)
library(data.table)
colnames(airlines_na)
data(airlines_na)
inTrain <- createDataPartition(y=airlines_na$ArrDelay, p=0.7, list=F) 
train_airlines<- airlines_na[inTrain, ] 
test_airlines<- airlines_na[-inTrain, ]
colnames(airlines_na)
#get character of the columns
char_col<- filter(data.table(v1=colnames(airlines_na),v2=sapply(as.data.table(airlines_na),class) ), v2=="character") %>% print

unique(airlines_na$UniqueCarrier) 
unique(airlines_na$TailNum)
unique(airlines_na$Origin)
unique(airlines_na$Dest)
unique(airlines_na$CancellationCode)

```
#### QUESTION 10

Now take the fitted regression and predict delays using the remaining data from the no-missing data set (this is the data you did not use in the fitting the model). Compare this to the actual delays and report the absolute mean error in your prediction. 
```{r}


```

